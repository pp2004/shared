| **Name**                        | **About**                                                                 | **Recent Event**                                                                                                                                                                                                                       |
|----------------------------------|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Training Data Poisoning**      | Attackers inject malicious data to manipulate model behavior.             | In 2024, attackers poisoned clinical LLMs by injecting trigger words into breast cancer treatment data, causing models to generate incorrect medical advice. This exposed vulnerabilities in healthcare AI systems.                    |
| **Model Extraction**             | Reverse-engineering proprietary models via systematic queries.            | In 2025, DeepSeek allegedly reverse-engineered ChatGPT’s outputs to train its open-source R1 model, raising concerns about IP theft and competitive risks for US-based AI firms. NCSU researchers also demonstrated hyperparameter theft from Google Edge TPUs in 2024. |
| **Membership Inference Attacks** | Determining if specific data was used in training, risking privacy leaks. | Researchers tricked ChatGPT into leaking training data (e.g., emails/phone numbers) by prompting it to repeat “poem” indefinitely. SPV-MIA attacks in 2024 achieved 90% accuracy in identifying training data from fine-tuned LLMs.      |
| **Supply Chain Attacks**         | Compromising AI dependencies (libraries, models) to insert backdoors.     | In 2025, SolarTrade’s logistics software was breached via AI-injected malicious updates, exposing payment data. Medtech’s pacemaker firmware was similarly compromised, endangering patient safety.                                     |
